# Neural Radiance Fields (NeRF)

## Introduction
This work is our implementation and analysis of Neural Radiance Fields(NeRF). This neural network is a fully connected network with input characterized by spatial positions (x, y, z) and viewing directions (θ, Φ), providing volume density and RGB pixel values corresponding to the viewing direction as outputs. NeRF uses classical volume rendering techniques, considering each point as a ray starting from the camera center and passing through each pixel to the world. The premise of the [paper](https://arxiv.org/abs/2003.08934) involves generating images of a scene from new viewpoints, a problem falling under novel image synthesis.

## Dependencies

To run the code, install the following necessary packages.

1. PyTorch
2. PIL
3. Numpy
4. tqdm
5. time
6. Matplotlib

## Usage

Follow the steps to run the code:

1. Set the Hyperparameters (Batch_Size, Learning_Rate, Step_Size, Epochs etc.) in Wrapper.py using command line arguments.
2. Set the appropriate paths for Training data in the Wrapper.py and FetchData.py.
3. Run the file Wrapper.py with appropriate arguments.
4. For testing, provide the appropriate paths for trained model, Rendered Image file generation and Gif generation in Test.py.
5. Run Test.py to generate the output.

## Dataset

Download the [dataset](https://drive.google.com/drive/folders/1JDdLGDruGNXWnM1eqY1FNL9PlStjaKWi). It contains three folders containing images: train, val, and test, and three files containing the orientation and position of the camera: `transforms_train.json`, `transforms_val.json`, and `transforms_test.json`. Each frame is a dictionary containing two keys, `transform_matrix` and `file_path`. The `file_path` is the path to the image (frame) under consideration, and the `transform_matrix` is the camera-to-world matrix for that image. We have considered the Lego and the Ship dataset.

## Method

- **Generate Rays and Sample Points:** In this step, we march rays through each pixel of the image. Next we sample points on the rays. These points are located on the rays, making them 3D points inside a cube of predefined dimensions inside which the 3D structure lies. This is done for each batch before providing as an input to the network. For training, we have considered 256 sample points on each ray lying between the near and far bounds (2 and 6 respectively). After getting these samples, we perturb the samples a little randomly which enables us to have a more even spread throughout the ray. These sample points are then passed to the model.
- **Positional Encoding:** The paper suggests that deep networks are biased toward learning low-frequency functions. To bypass this problem NeRF proposes mapping the input vector to a higher dimensional representation.
- **Deep Learning:** We pass these points into a Multi-Layer Perceptron (MLP) and predict the color and density corresponding to that point.
- **Volume Rendering:** After we have the color and density of each point, we apply classical volume rendering to predict the color of the image pixel through which the ray passes.

## Network
We have used the network mentioned in the official NeRF paper but we have only used one network instead of both coarse and fine networks. The implementation is a fully connected network and has hidden layers with 256 channels each and a ReLU after it. After 4 layers, there is a skip connection that concatenates the input to the fifth layer. An additional layer outputs the density and is then concatenated with the viewing direction. It is then processed by an additional fully connected layer with 128 channels which gives the RGB output. For dataloading, we read all the images of the train or test set provided and convert them to a lower resolution (200x200) for faster computation. We then generate rays for all the images, convert them to 2D tensors and provide them to the torch dataloader which will provide all the values according to the batch size as an input for the network. In the network, firstly, all sample points are calculated for each ray provided by the dataloader, positional encoding is done and then the data is given to the network. The MSE loss function is used to calculate the loss based on the output generated by the network. Renderer is used to render the output generated by the network.


<p float="left">
 <img src="Images/Network.png" width="800"/>
</p>
<p align="center">*NeRF Network*</p>

- **Number of samples per ray:** 256
- **Learning rate:** 0.0005
- **Optimizer:** Adam
- **Loss:** MSE loss
- **Near, Far bounds:** 2, 6

## Outputs

The outputs of the Test set for the Lego dataset can be seen in the following figure. All of the rendered images are novel views that are not encountered by the network while training. The rendered images are compared with the ground truth and it is evident that the network does an adequate job of reproducing the novel views.

<p float="left">
 <img src="Images/Lego/GT/r_21.png" width="400"/>
 <img src="Images/Lego/Rendered/Rendered_Image_21.png" width="400"/>
 <img src="Images/Lego/GT/r_62.png" width="400"/>
 <img src="Images/Lego/Rendered/Rendered_Image_62.png" width="400"/>
 <img src="Images/Lego/GT/r_95.png" width="400"/>
 <img src="Images/Lego/Rendered/Rendered_Image_95.png" width="400"/>
</p>
<p align="center">*Ground Truth (Left) and Rendered Image(Right)*</p>

The results shown above are obtained with implementation of positional encoding as mentioned in the original paper. To compare the effects of positional encoding, we trained the same model for the same number of epochs without positional encoding. The outputs obtained are less than desirable and the comparision can be seen in the following figure.

<p float="left">
 <img src="Images/Lego/PE/Screenshot%20from%202024-03-11%2017-30-15.png" width="400"/>
 <img src="Images/Lego/PE/Screenshot%20from%202024-03-11%2017-31-11.png" width="400"/>
</p>
<p align="center">*Rendered images without positional encoding (left) and with positional encoding (Right)*</p>



Similarly, the output images for the ship dataset are shown in the following figure.

<p float="left">
 <img src="Images/Ship/GT/r_12.png" width="400"/>
 <img src="Images/Ship/Rendered/Rendered_Image_12.png" width="400"/>
 <img src="Images/Ship/GT/r_143.png" width="400"/>
 <img src="Images/Ship/Rendered/Rendered_Image_143.png" width="400"/>
 <img src="Images/Ship/GT/r_163.png" width="400"/>
 <img src="Images/Ship/Rendered/Rendered_Image_163.png" width="400"/>
</p>
<p align="center">*Ground Truth (Left) and Rendered Image(Right)*</p>

## Observations
It is observed that with the same pretrained model, when the test images are queried with different sample points per rays, the output changes drastically. This can be seen in the following figure.

<p float="left">
 <img src="Images/Lego/Rendered_Image_21.png" width="400"/>
 <img src="Images/Lego/Rendered/Rendered_Image_21.png" width="400"/>
</p>
<p align="center">*Rendered images with 32 sampling points (left) and 128 sampling points (right) per ray*</p>

Another observation is that there are some rendered images which have artifacts that are undesirable. These can be seen in the following figure.

<p float="left">
 <img src="Images/Ship/Bad/r_98.png" width="400"/>
 <img src="Images/Ship/Bad/Rendered_Image_98.png" width="400"/>
</p>
<p align="center">*Bad rendering of images with artifacts*</p>

Overall, looking at the Lego and the Ship rendered outputs, it can be seen that the results are adequate but can be improved. There are 2 main improvements that can be done here:

1. The image resolution is reduced to (200x200) from the original (800x800). This is done to reduce the training time but as a result of this, the output resolution is less so the image does not have the sharpness and clarity that is expected.
2. The training can be done for more epochs to improve the output and to get rid of the artifacts that have been generated.



## Author
- **Dhrumil Kotadia**  
  Robotics Engineering Department, Worcester Polytechnic Institute  
