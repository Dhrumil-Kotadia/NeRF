# NeRF

### Introduction
In this work, we provide a comprehensive analysis of our implementation of the NeRF network. NeRF, short for Neural Radiance Fields, embodies a sophisticated framework encapsulated within a fully connected network architecture. This neural network operates seamlessly with its input characterized by a unified continuous 5D coordinate system encompassing spatial positions (x, y, z) and viewing directions (θ, Φ), providing volume density and RGB pixel values corresponding to the viewing direction as outputs. We use the lego and ship datasets from the official dataset obtained from the original author. NeRF uses classical volume rendering techniques, considering each point as a ray starting from the camera center and passing through the pixel to the world. The premise of the [paper](https://arxiv.org/abs/2003.08934) involves generating images of a scene from new viewpoints, a problem falling under novel image synthesis.

### Background and Dataset

Download the [dataset](https://drive.google.com/drive/folders/1JDdLGDruGNXWnM1eqY1FNL9PlStjaKWi). It contains three folders containing images: train, val, and test, and three files containing the orientation and position of the camera: `transforms_train.json`, `transforms_val.json`, and `transforms_test.json`. Each frame is a dictionary containing two keys, `transform_matrix` and `file_path`. The `file_path` is the path to the image (frame) under consideration, and the `transform_matrix` is the camera-to-world matrix for that image.

### Method

- **Generate Rays and Sample Points:** In this step, we march rays through each pixel of the image. Next we sample points on the rays. These points are located on the rays, making them 3D points inside a cube of predefined dimensions inside which the 3D structure lies. This is done for each batch before providing as an input to the network. For training, we have considered 256 sample points on each ray lying between the near and far bounds (2 and 6 respectively). After getting these samples, we perturb the samples a little randomly which enables us to have a more even spread throughout the ray. These sample points are then passed to the model.
- **Positional Encoding:** The paper suggests that deep networks are biased toward learning low-frequency functions. To bypass this problem NeRF proposes mapping the input vector to a higher dimensional representation.
- **Deep Learning:** We pass these points into a Multi-Layer Perceptron (MLP) and predict the color and density corresponding to that point.
- **Volume Rendering:** After we have the color and density of each point, we apply classical volume rendering to predict the color of the image pixel through which the ray passes.

### Network
We have used the network mentioned in the official NeRF paper but we have only used one network instead of both coarse and fine networks. The implementation is a fully connected network and has hidden layers with 256 channels each and a ReLU after it. After 4 layers, there is a skip connection that concatenates the input to the fifth layer. An additional layer outputs the density and is then concatenated with the viewing direction. It is then processed by an additional fully connected layer with 128 channels which gives the RGB output. For dataloading, we read all the images of the train or test set provided and convert them to a lower resolution (200x200) for faster computation. We then generate rays for all the images, convert them to 2D tensors and provide them to the torch dataloader which will provide all the values according to the batch size as an input for the network. In the network, firstly, all sample points are calculated for each ray provided by the dataloader, positional encoding is done and then the data is given to the network. The MSE loss function is used to calculate the loss based on the output generated by the network. Renderer is used to render the output generated by the network.

- **Number of samples per ray:** 256
- **Learning rate:** 0.0005
- **Optimizer:** Adam
- **Loss:** MSE loss
- **Near, Far bounds:** 2, 6
![NeRF Network](Images/Network.png)



### Outputs

The outputs of the Test set for the Lego dataset can be seen in figure 4. The rendered images are compared with the ground truth and it is evident that the network does an adequate job of reproducing the novel views.

<figure>
 <img src="Images/Lego/GT/r_21.png" width="200">
 <img src="Images/Lego/Rendered/Rendered_Image_21.png" width="200">
 <figcaption>Ground Truth (Left) and Rendered Image</figcaption>
</figure>
![Ground Truth (Left) and Rendered Image (Right)](Images/Lego/GT/r_21.png | width = 200) ![Ground Truth (Left) and Rendered Image (Right)](Images/Lego/Rendered/Rendered_Image_21.png | width = 200)  
![Ground Truth (Left) and Rendered Image (Right)](Images/Lego/GT/r_62.png) ![Ground Truth (Left) and Rendered Image (Right)](Images/Lego/Rendered/Rendered_Image_62.png)  
![Ground Truth (Left) and Rendered Image (Right)](Images/Lego/GT/r_95.png) ![Ground Truth (Left) and Rendered Image (Right)](Images/Lego/Rendered/Rendered_Image_95.png)

However, this is done using positional encoding. Without positional encoding, the outputs are less than desirable as can be seen in figure 5.

![Rendered images without positional encoding (left) and with positional encoding (Right)](Images/Lego/PE/Screenshot%20from%202024-03-11%2017-30-15.png) ![Rendered images without positional encoding (left) and with positional encoding (Right)](Images/Lego/PE/Screenshot%20from%202024-03-11%2017-31-11.png)

The output images for the ship dataset are shown in figure 6.

![Ground Truth (Left) and Rendered Image (Right)](Images/Ship/GT/r_12.png) ![Ground Truth (Left) and Rendered Image (Right)](Images/Ship/Rendered/Rendered_Image_12.png)  
![Ground Truth (Left) and Rendered Image (Right)](Images/Ship/GT/r_143.png) ![Ground Truth (Left) and Rendered Image (Right)](Images/Ship/Rendered/Rendered_Image_143.png)  
![Ground Truth (Left) and Rendered Image (Right)](Images/Ship/GT/r_163.png) ![Ground Truth (Left) and Rendered Image (Right)](Images/Ship/Rendered/Rendered_Image_163.png)

### Observations
An observation is that with the same pretrained model, when the test images are queried with different sample points per rays, the output changes drastically. This can be seen in figure 7.

![Rendered images with 32 sampling points (left) and 128 sampling points (right) per ray](Images/Lego/Rendered_Image_21.png) ![Rendered images with 32 sampling points (left) and 128 sampling points (right) per ray](Images/Lego/Rendered/Rendered_Image_21.png)

Another observation is that there are some rendered images which have artifacts that are undesirable. These can be seen in figure 8.

![Bad rendering of images with artifacts](Images/Ship/Bad/r_98.png) ![Bad rendering of images with artifacts](Images/Ship/Bad/Rendered_Image_98.png)

Overall, looking at the Lego and the Ship rendered outputs, it can be seen that the results are adequate but can be improved. There are 2 main improvements that can be done here:

1. The image resolution is reduced to (200x200) from the original (800x800). This is done to reduce the training time but as a result of this, the output resolution is less so the image does not have the sharpness and clarity that is expected.
2. The training can be done for more epochs to improve the output and to get rid of the artifacts that have been generated.

 

## Installation

To run the code, install all the necessary packages.

1. PyTorch
2. PIL
3. Numpy
4. tqdm
5. time
6. Matplotlib

## Usage

Follow the steps to run the code:

1. Set the Hyperparameters (Batch_Size, Learning_Rate, Step_Size, Epochs etc.) in Wrapper.py using command line arguments.
2. Set the appropriate paths for Training data in the Wrapper.py and FetchData.py.
3. Run the file Wrapper.py.
4. For testing, provide the appropriate paths for trained model, Rendered Image file generation and Gif generation in Test.py.
5. Run Test.py to generate the output.


### Author
- **Dhrumil Kotadia**  
  Robotics Engineering Department  
  Worcester Polytechnic Institute  
  Worcester, Massachusetts
